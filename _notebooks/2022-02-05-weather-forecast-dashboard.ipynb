{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking 48 Mountain Weather Locations at Once\n",
    "> Using Async Python to feed a Streamlit Dashboard\n",
    "\n",
    "- toc: true \n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: [streamlit, python, async, intermediate]\n",
    "- image: images/weather.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Peak Weather: Checking New Hampshire's 48 4,000 Footers\n",
    "\n",
    "Check it out [live on streamlit cloud](https://share.streamlit.io/gerardrbentley/peak-weather/main/streamlit_app/streamlit_app.py)\n",
    "\n",
    "Built to give you a dashboard view of the next few hours' forecast for New Hampshires 48 4,000 ft mountains.\n",
    "Gonna rain on the Kinsmans?\n",
    "Is it snowing on Washington?\n",
    "Should I hike Owl's Head?\n",
    "\n",
    "Powered by [Streamlit](https://docs.streamlit.io/) + [Open Weather API](https://openweathermap.org/api).\n",
    "Specifically, Streamlit runs the web interactinos and OpenWeather provides the data.\n",
    "\n",
    "This post will go over a few aspects of the app:\n",
    "\n",
    "- Data scraping the mountain metadata\n",
    "- Connecting to Weather API feed\n",
    "- Making it reasonably fast \n",
    "\n",
    "## Data Scraping\n",
    "\n",
    "I couldn't find an easy csv or api for the latitudes and longitudes of the 48 4,000 footers, so I turned to [Wikipedia](https://en.wikipedia.org/wiki/Four-thousand_footers) for the list.\n",
    "\n",
    "### Try Pandas\n",
    "\n",
    "The [`read_html()`](https://pandas.pydata.org/docs/reference/api/pandas.read_html.html) function in Pandas has been a sanity saver in my job for reading data from flat file specification documents.\n",
    "\n",
    "Unfortunately the data I'm looking for in Wikipedia is in `<li>...</li>` tags, not a real html `<table>...</table>`\n",
    "\n",
    "### Naive Copy+Paste\n",
    "\n",
    "Next I tried just copying the list of names and heights to feed to a search API, yielding a csv like the following after some cleanup:\n",
    "\n",
    "```txt\n",
    "name,height_ft\n",
    "Washington,6288\n",
    "Adams,5774\n",
    "Jefferson,5712\n",
    "```\n",
    "\n",
    "And this gives us csv access to the data like so:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "mountains = pd.read_csv('./data/mtns.txt')\n",
    "mountains.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A-Links to the Rescue\n",
    "\n",
    "Now with the list of peaks, I needed the corresponding latitude and longitudes.\n",
    "\n",
    "After searching for a straightforward source, I realized the Wikipedia pages linked from the main list page were the best...\n",
    "\n",
    "I grabbed the portion of the html with the list to a file with dev tools (chrome f12), but could have been done with BeautifulSoup\n",
    "\n",
    "#### Scrape Mountain Links\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "# Chunk from 4,000 footers page containing list of mountains\n",
    "# https://en.wikipedia.org/wiki/Four-thousand_footers\n",
    "soup = BeautifulSoup(open(\"./data/wiki.html\"), \"html.parser\")\n",
    "\n",
    "# Gather <a> tags, ignore citation\n",
    "links = [x for x in soup.find_all(\"a\") if x.get(\"title\")]\n",
    "links[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Lat Lon For One Mountain\n",
    "\n",
    "With access to the `href` attributes of the `<a>` tags, I could then fetch all of those pages and scrape out the Lat and Lon from each.\n",
    "\n",
    "Most older guides will use Python's `requests` library for this kind of task, but that library does not have the ability to send asynchronous requests without multiprocessing (Translation: It's difficult to fetch a bunch of pages all at once).\n",
    "\n",
    "I've found success with [`httpx`](https://www.python-httpx.org/) and [`aiohttp`](https://docs.aiohttp.org/en/stable/) for making asynchronous requests in one Python process.\n",
    "So I went with `httpx` for fetching each page.\n",
    "\n",
    "Lets demonstrate fetching one of those pages and scraping the Latitude and Longitude.\n",
    "We won't worry too much about errors or missed data for this cleaning phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import httpx\n",
    "# English Wikipedia\n",
    "BASE_URL = \"https://en.wikipedia.org\"\n",
    "\n",
    "def convert(raw_tude: str) -> float:\n",
    "    \"\"\"Takes a wikipedia latitude or longitude string and converts it to float\n",
    "    Math Source: https://stackoverflow.com/questions/21298772/how-to-convert-latitude-longitude-to-decimal-in-python\n",
    "\n",
    "    Args:\n",
    "        raw_tude (str): Lat or Lon in one of the following forms:\n",
    "            degrees°minutes′seconds″N,\n",
    "            degrees°minutes′N,\n",
    "            degrees-minutes-secondsN,\n",
    "            degrees-minutesN\n",
    "\n",
    "    Returns:\n",
    "        (float): Float converted lat or lon based on supplied DMS\n",
    "    \"\"\"\n",
    "    tude = raw_tude.replace(\"°\", \"-\").replace(\"′\", \"-\").replace(\"″\", \"\")\n",
    "    if tude[-2] == \"-\":\n",
    "        tude = tude[:-2] + tude[-1]\n",
    "    multiplier = 1 if tude[-1] in [\"N\", \"E\"] else -1\n",
    "    return multiplier * sum(\n",
    "        float(x) / 60 ** n for n, x in enumerate(tude[:-1].split(\"-\"))\n",
    "    )\n",
    "\n",
    "a_link = links[0]\n",
    "a_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bs4 lets us \"get\" html tag attributes as in python dicts\n",
    "name = a_link.get(\"title\")\n",
    "link = a_link.get(\"href\")\n",
    "\n",
    "# httpx lets us fetch the raw html page\n",
    "raw_page = httpx.get(BASE_URL + link)\n",
    "# Which bs4 will help parse\n",
    "raw_soup = BeautifulSoup(raw_page, \"html.parser\")\n",
    "\n",
    "# find returns first instance of a tag with this class\n",
    "raw_lat = raw_soup.find(class_=\"latitude\").text.strip()\n",
    "lat = convert(raw_lat)\n",
    "raw_lon = raw_soup.find(class_=\"longitude\").text.strip()\n",
    "lon = convert(raw_lon)\n",
    "\n",
    "name, link, lat, lon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Lat Lon For Many Mountains\n",
    "\n",
    "Lets chuck the first 10 mountains into a for-loop and fetch the same pieces of data.\n",
    "\n",
    "First we'll define a function to encapsulate the synchronous fetch logic\n",
    "\n",
    "Then we'll see how long this takes with jupyter's `%%time` magic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sync_get_coords(a_link: BeautifulSoup) -> dict:\n",
    "    name = a_link.get(\"title\")\n",
    "    link = a_link.get(\"href\")\n",
    "    raw_page = httpx.get(BASE_URL + link)\n",
    "    raw_soup = BeautifulSoup(raw_page, \"html.parser\")\n",
    "    raw_lat = raw_soup.find(class_=\"latitude\").text.strip()\n",
    "    lat = convert(raw_lat)\n",
    "    raw_lon = raw_soup.find(class_=\"longitude\").text.strip()\n",
    "    lon = convert(raw_lon)\n",
    "    return {\"name\": name, \"link\": link, \"lat\": lat, \"lon\": lon}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "for a_link in links[:10]:\n",
    "    result = sync_get_coords(a_link)\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results will vary by machine, internet connection, Wikipedia server status, and [butterly wing flaps](https://xkcd.com/378/).\n",
    "\n",
    "Mine were like this the first time around:\n",
    "\n",
    "```txt\n",
    "CPU times: user 2.25 s, sys: 65.1 ms, total: 2.31 s\n",
    "Wall time: 5.47 s\n",
    "```\n",
    "\n",
    "#### Faster Fetching\n",
    "\n",
    "We're not using the asynchronous capabilities of `httpx` yet, so each of the 10 requests to Wikipedia needs to go over the wire and back in order for the next request to start.\n",
    "\n",
    "How about we speed things up a little (Jupyter `%%time` doesn't work on async cells):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "async def get_coords(client: httpx.AsyncClient, a_link: BeautifulSoup) -> dict:\n",
    "    \"\"\"Given http client and <a> link from wikipedia list,\n",
    "    Fetches the place's html page,\n",
    "    Attempts to parse and convert lat and lon to decimal from the page (first occurrence)\n",
    "    Returns entry with keys: \"name\", \"link\", \"lat\", \"lon\"\n",
    "\n",
    "    Args:\n",
    "        client (httpx.AsyncClient): To make requests. See httpx docs\n",
    "        a_link (BeautifulSoup): <a> ... </a> chunk\n",
    "\n",
    "    Returns:\n",
    "        dict: coordinate entry for this wikipedia place\n",
    "    \"\"\"    \n",
    "    name = a_link.get(\"title\")\n",
    "    link = a_link.get(\"href\")\n",
    "    raw_page = await client.get(BASE_URL + link)\n",
    "    raw_soup = BeautifulSoup(raw_page, \"html.parser\")\n",
    "    raw_lat = raw_soup.find(class_=\"latitude\").text.strip()\n",
    "    lat = convert(raw_lat)\n",
    "\n",
    "    raw_lon = raw_soup.find(class_=\"longitude\").text.strip()\n",
    "    lon = convert(raw_lon)\n",
    "\n",
    "    return {\"name\": name, \"link\": link, \"lat\": lat, \"lon\": lon}\n",
    "\n",
    "\n",
    "async def gather_coords(links: list) -> list:\n",
    "    \"\"\"Given List of a links, asynchronously fetch all of them and return results\"\"\"\n",
    "    async with httpx.AsyncClient() as client:\n",
    "        tasks = [asyncio.ensure_future(get_coords(client, link)) for link in links]\n",
    "        coords = await asyncio.gather(*tasks)\n",
    "        return coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeit import default_timer as timer\n",
    "start = timer()\n",
    "# Async get all lat lon as list of dictionaries\n",
    "coords = await gather_coords(links[:10])\n",
    "end = timer()\n",
    "print(*coords[:10], f\"{end - start :.2f} seconds\", sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2.16 / 5.47"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "40% of the time spent scraping data, sounds good to me!\n",
    "\n",
    "#### Data Cleaning\n",
    "\n",
    "If you thought the \"finds first occurrence\" strategy for scraping latitude and longitude was going to cause errors, cheers to you.\n",
    "\n",
    "Turns out just a few mountains have multiple peaks that count as 4,000 footers, so these mountains have 2 sets of latitudes and longitudes.\n",
    "\n",
    "I fetched these by hand and said LGTM with my csv of:\n",
    "- Mountain Names\n",
    "- Heights\n",
    "- Latitudes\n",
    "- Longitudes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weather Scraping\n",
    "\n",
    "I figured there's probably a free open API for accessing weather data, and a quick google found two that caught my eye:\n",
    "\n",
    "- [OpenWeatherMap](https://openweathermap.org/api)\n",
    "- [Weather.gov](https://www.weather.gov/documentation/services-web-api)\n",
    "\n",
    "It's a free API, but this was the selling point for OpenWeatherMap for this Proof-of-Concept project:\n",
    "\n",
    "The [`One Call API`](https://openweathermap.org/api/one-call-api) provides the following weather data for any geographical coordinates:\n",
    "\n",
    "- *Current weather*\n",
    "- *Minute forecast* for 1 hour\n",
    "- *Hourly forecast* for 48 hours\n",
    "- *Daily forecast* for 7 days\n",
    "- National weather *alerts*\n",
    "- *Historical* weather data for the previous 5 days\n",
    "\n",
    "### API Signup and Prep\n",
    "\n",
    "Getting a free account and key was straightforward involving just an email address verification link.\n",
    "\n",
    "Then off to the races with the following documentation (there's more on their site in better formatting):\n",
    "\n",
    "```sh\n",
    "# One Call URL\n",
    "https://api.openweathermap.org/data/2.5/onecall?lat={lat}&lon={lon}&exclude={part}&appid={API key}\n",
    "```\n",
    "\n",
    "**Parameters**\n",
    "\n",
    "`lat`, `lon`: *required* \n",
    "Geographical coordinates (latitude, longitude)\n",
    "\n",
    "`appid`: *required* \n",
    "Your unique API key (you can always find it on your account page under the \"API key\" tab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseSettings\n",
    "\n",
    "\n",
    "class Settings(BaseSettings):\n",
    "    \"\"\"Handles fetching configuration from environment variables and secrets.\n",
    "    Type-hinting for config as a bonus\"\"\"\n",
    "\n",
    "    open_weather_api_key: str\n",
    "\n",
    "\n",
    "settings = Settings()\n",
    "\n",
    "\n",
    "class WeatherUnit:\n",
    "    STANDARD = \"standard\"\n",
    "    KELVIN = \"standard\"\n",
    "    METRIC = \"metric\"\n",
    "    IMPERIAL = \"imperial\"\n",
    "\n",
    "\n",
    "def get_one_call_endpoint(\n",
    "    lat: float,\n",
    "    lon: float,\n",
    "    units: WeatherUnit = WeatherUnit.IMPERIAL,\n",
    "    exclude=\"\",\n",
    "    lang=\"en\",\n",
    "):\n",
    "    if exclude != \"\":\n",
    "        exclude = f\"&exclude={exclude}\"\n",
    "    return f\"https://api.openweathermap.org/data/2.5/onecall?lat={lat}&lon={lon}&units={units}{exclude}&lang={lang}&appid={settings.open_weather_api_key}\"\n",
    "\n",
    "\n",
    "def get_one_call_data(lat, lon):\n",
    "    endpoint = get_one_call_endpoint(lat, lon)\n",
    "    print(f\"Fetching from '{endpoint}'\")\n",
    "    response = httpx.get(endpoint)\n",
    "    return response.json()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test One Location\n",
    "\n",
    "I included some of the API parameters as endpoint configuration options as I messed around with it.\n",
    "\n",
    "For this use case these defaults are sensible to me:\n",
    "\n",
    "- American users -> `units = Imperial`\n",
    "- English speaking users -> `lang=\"en\"`\n",
    "- Exclude -> don't care too much about some extra data coming over to the server\n",
    "\n",
    "Lets see what we get for a live mountain location!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mount_washington = coords[0]\n",
    "mount_washington"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_one_call_data(mount_washington['lat'], mount_washington['lon'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch for Many Locations\n",
    "\n",
    "Using the same scaffolding as the Wikipedia asynchronous scrape, the helper code for the main streamlit app also relies on `httpx` to fetch 48 responses quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "async def async_get_one_call_data(client: httpx.AsyncClient, lat: float, lon: float) -> dict:\n",
    "    \"\"\"Given http client and valid lat lon, retrieves open weather \"One call\" API data\n",
    "\n",
    "    Args:\n",
    "        client (httpx.AsyncClient): To make requests. See httpx docs\n",
    "        lat (float): lat of the desired location\n",
    "        lon (float): lon of the desired location\n",
    "\n",
    "    Returns:\n",
    "        dict: json response from Open Weather One Call\n",
    "    \"\"\"\n",
    "    endpoint = get_one_call_endpoint(lat, lon)\n",
    "    response = await client.get(endpoint)\n",
    "    return response.json()\n",
    "\n",
    "\n",
    "async def gather_one_call_weather_data(lat_lon_pairs: list) -> list:\n",
    "    \"\"\"Given list of tuples of lat, lon pairs, will asynchronously fetch the one call open weather api data for those pairs\n",
    "\n",
    "    Args:\n",
    "        lat_lon_pairs (list): Destinations to get data for\n",
    "\n",
    "    Returns:\n",
    "        list: List of dictionaries which are json responses from open weather\n",
    "    \"\"\"\n",
    "    async with httpx.AsyncClient() as client:\n",
    "        tasks = [\n",
    "            asyncio.ensure_future(async_get_one_call_data(client, lat, lon))\n",
    "            for lat, lon in lat_lon_pairs\n",
    "        ]\n",
    "        one_call_weather_data = await asyncio.gather(*tasks)\n",
    "        return one_call_weather_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web App Component\n",
    "\n",
    "Goals from the start:\n",
    "- Usable UI for comparing / viewing weather on 48 locations (mobile-friendly for hikers)\n",
    "- Not sluggish to load data or click through page after page to get different mountains / times\n",
    "- Good uptime\n",
    "\n",
    "Other technical considerations:\n",
    "- Obeying API limits\n",
    "    - API key security\n",
    "- Streamlit resource limits\n",
    "    - Cloud host or self host\n",
    "\n",
    "### Caching Data\n",
    "\n",
    "There are 2 main points of loading data in the app:\n",
    "\n",
    "- Load the list of mountains, heights, lats, lons\n",
    "- Fetch live data from OpenWeatherMap for all locations\n",
    "\n",
    "With Streamlit, decorating a function with `@st.cache()` will save the computed result so that it can be loaded faster by the next user!\n",
    "\n",
    "#### Caching Mountain Data\n",
    "\n",
    "The first list is static, and purely for convenience of fetching columns I load it in with `pandas`. (In hindsight I could have at least reset the index after sorting).\n",
    "\n",
    "Leaving the default arguments lets this dataset get cached indefinitely (until the app gets shut down / restarted)\n",
    "\n",
    "*note:* `st.cache` decorators commented out in notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# import streamlit as st\n",
    "\n",
    "#@st.cache()\n",
    "def load_metadata() -> pd.DataFrame:\n",
    "    \"\"\"Function to read mountain lat, lon, and other metadata and cache results\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: df containing information for 48 mountains\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(\"./data/mountains.csv\")\n",
    "    df = df.sort_values(\"name\")\n",
    "    return df\n",
    "\n",
    "load_metadata().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Caching Weather Data\n",
    "\n",
    "With this dataset I don't want to cache things indefinitely.\n",
    "In fact, we want it to update as often as the API limits will allow us to query it!\n",
    "\n",
    "Setting a `ttl` or \"Time To Live\" value in `st.cache(ttl=...)` will cause the cache to bust if the precomputed result is longer than the provided time.\n",
    "\n",
    "We'll set the `ttl` to 60 minutes to respect OpenWeatherMaps.\n",
    "\n",
    "This means that if 100 users all open the app within 59 minutes of one another then only 1 request to `load_data()` would actually go to OpenWeatherMaps. The other 99 requests would use the cached result.\n",
    "\n",
    "When any user opens it 61 minutes after the first user, the cache will be busted and another request to OpenWeatherMaps will refresh all of the 48 mountains' weather data in the app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@st.cache(ttl=60 * 60)\n",
    "def load_data(lat_lon_pairs: list) -> list:\n",
    "    \"\"\"Function to fetch Open Weather data and cache results\n",
    "\n",
    "    Args:\n",
    "        lat_lon_pairs (list): Destinations to get data for\n",
    "\n",
    "    Returns:\n",
    "        list: List of dictionaries which are json responses from open weather\n",
    "    \"\"\"\n",
    "    data = asyncio.run(gather_one_call_weather_data(lat_lon_pairs))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonuses\n",
    "\n",
    "#### Display future forecast\n",
    "\n",
    "Hikers don't need to know just the weather right now.\n",
    "They also need to know the next few hours' forecast.\n",
    "\n",
    "The OpenWeatherMaps data provides temperature and weather event forecasts hourly.\n",
    "\n",
    "So how about a row across the screen with 5 hours of data in 5 even columns.\n",
    "\n",
    "Feels good on desktop, but a horrendous amount of scrolling past locations you don't care about on mobile.\n",
    "\n",
    "`st.expander()` provides a way to tuck sections away in a drop down hide/expand section.\n",
    "\n",
    "Then using `st.columns()` we can get an iterator over `x` amount of columns.\n",
    "Zipping this with the hourly results starting from the next hour gives a nice way to match up layout to data.\n",
    "It also gives some flexibility for how many columns to include.\n",
    "\n",
    "```py\n",
    "response = load_data()[0]\n",
    "current_temperature = round(response[\"current\"][\"temp\"], 1)\n",
    "\n",
    "with st.expander(\"Expand for future forecast:\"):\n",
    "    for col, entry in zip(st.columns(5), response[\"hourly\"][1:]):\n",
    "        col.write(f\"{clean_time(entry['dt'])}\")\n",
    "        \n",
    "        temperature = round(entry[\"temp\"], 1)\n",
    "        col.metric(\n",
    "            \"Temp (F)\", temperature, round(temperature - current_temperature, 1)\n",
    "        )\n",
    "        current_temperature = temperature\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Jump Link Table\n",
    "\n",
    "Using the app on mobile even with expander sections was too much scrolling.\n",
    "\n",
    "I thought a Markdown table of links would be more straightforward, but I would up doing a bunch of string mangling to get it running.\n",
    "\n",
    "Having anchors on most commands such as `st.title()` is great for in-page navigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mtn_anchor(mountain: str) -> str:\n",
    "    anchor = mountain.lower().replace(\" \", \"-\")\n",
    "    return f\"[{mountain}](#{anchor})\"\n",
    "\n",
    "mountains = load_metadata()\n",
    "\n",
    "table = []\n",
    "\n",
    "table.append(\"| Mountains |  |  |\")\n",
    "table.append(\"|---|---|---|\")\n",
    "for left, middle, right in zip(\n",
    "    mountains.name[::3], mountains.name[1::3], mountains.name[2::3]\n",
    "):\n",
    "    table.append(\n",
    "        f\"| {get_mtn_anchor(left)} | {get_mtn_anchor(middle)} | {get_mtn_anchor(right)} |\"\n",
    "    )\n",
    "# st.markdown(\"\\n\".join(table))\n",
    "\"\\n\".join(table)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "73e1358792c9ac8aceb57c7078b4fca0c4e1fc77d5bda5a6697c2327ec5098c9"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 ('py310')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
