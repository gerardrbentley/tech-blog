{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Holy ðŸ¦†uck! Fast Analysis with DuckDB + Pyarrow\n",
    "> Trying out some new speedy tools for data analysis\n",
    "\n",
    "- toc: true \n",
    "- badges: true\n",
    "- categories: [python, data, intermediate]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Holy ðŸ¦†uck! Fast Analysis with DuckDB + Pyarrow\n",
    "\n",
    "Turning to DuckDB when you need to crunch more numbers faster than pandas in your Streamlit app ðŸŽˆ\n",
    "\n",
    "Inspired by \"DuckDB quacks Arrow\" blogpost cross-posted on [duckdb](https://duckdb.org/2021/12/03/duck-arrow.html) and [arrow](https://arrow.apache.org/blog/2021/12/03/arrow-duckdb/)\n",
    "\n",
    "## Background\n",
    "\n",
    "`streamlit` and Streamlit Cloud are fantastic for sharing your data exploration apps.\n",
    "A very common pattern uses csv files with `pandas` to accomplish the necessary steps of:\n",
    "\n",
    "- Load the data into the program\n",
    "- Filter data by certain columns or attributes\n",
    "- Compute analyses on the data (averages, counts, etc.)\n",
    "\n",
    "## NYC Uber Data\n",
    "\n",
    "Let's take this NYC Uber dataset example from Streamlit.\n",
    "We'll pay attention to:\n",
    "\n",
    "- How much RAM / memory is used\n",
    "- How long it takes to perform each step\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# import streamlit as st\n",
    "\n",
    "# singleton ignored because we're not in streamlit anymore\n",
    "# @st.experimental_singleton\n",
    "def load_data():\n",
    "    data = pd.read_csv(\n",
    "        \"uber-raw-data-sep14.csv.gz\",\n",
    "        nrows=100000,  # approx. 10% of data\n",
    "        names=[\n",
    "            \"date/time\",\n",
    "            \"lat\",\n",
    "            \"lon\",\n",
    "        ],  # specify names directly since they don't change\n",
    "        skiprows=1,  # don't read header since names specified directly\n",
    "        usecols=[0, 1, 2],  # doesn't load last column, constant value \"B02512\"\n",
    "        parse_dates=[\n",
    "            \"date/time\"\n",
    "        ],  # set as datetime instead of converting after the fact\n",
    "    )\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.29 s, sys: 33 ms, total: 3.33 s\n",
      "Wall time: 3.32 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100000 entries, 0 to 99999\n",
      "Data columns (total 3 columns):\n",
      " #   Column     Non-Null Count   Dtype         \n",
      "---  ------     --------------   -----         \n",
      " 0   date/time  100000 non-null  datetime64[ns]\n",
      " 1   lat        100000 non-null  float64       \n",
      " 2   lon        100000 non-null  float64       \n",
      "dtypes: datetime64[ns](1), float64(2)\n",
      "memory usage: 2.3 MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feel free to reference the `read_csv` [documentation](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html), the focus of this post is on the `nrows=100000` argument though.\n",
    "\n",
    "This `nrows` is used to limit the number of rows that get loaded into our application.\n",
    "Taking in `100,000` rows landed us around `2.3 MB` of memory allocation for the data.\n",
    "\n",
    "It loaded on my computer in `~3` seconds.\n",
    "\n",
    "Let's see how that would go without our `nrows` limitation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_full_data():\n",
    "    data = pd.read_csv(\n",
    "        \"uber-raw-data-sep14.csv.gz\",\n",
    "        # nrows=100000,  # approx. 10% of data\n",
    "        names=[\n",
    "            \"date/time\",\n",
    "            \"lat\",\n",
    "            \"lon\",\n",
    "        ],  # specify names directly since they don't change\n",
    "        skiprows=1,  # don't read header since names specified directly\n",
    "        usecols=[0, 1, 2],  # doesn't load last column, constant value \"B02512\"\n",
    "        parse_dates=[\n",
    "            \"date/time\"\n",
    "        ],  # set as datetime instead of converting after the fact\n",
    "    )\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 33.6 s, sys: 243 ms, total: 33.8 s\n",
      "Wall time: 33.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "full_data = load_full_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1028136 entries, 0 to 1028135\n",
      "Data columns (total 3 columns):\n",
      " #   Column     Non-Null Count    Dtype         \n",
      "---  ------     --------------    -----         \n",
      " 0   date/time  1028136 non-null  datetime64[ns]\n",
      " 1   lat        1028136 non-null  float64       \n",
      " 2   lon        1028136 non-null  float64       \n",
      "dtypes: datetime64[ns](1), float64(2)\n",
      "memory usage: 23.5 MB\n"
     ]
    }
   ],
   "source": [
    "full_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so with `~10` times as much data (`1,028,136` vs `100,000`) we use:\n",
    "\n",
    "- `~10` times as much memory (`23.5 MB` vs `2.3 MB`)\n",
    "- `~10` times as much time (`30.1 s` vs `2.99 s`)\n",
    "\n",
    "The first time this app loads in `streamlit` will be a bit slow either way, but the `singleton` decorator is designed to prevent having to re-compute objects like this.\n",
    "\n",
    "(Also note that this is a single month of data... a year might include `~12,337,632` entries based on this september 2014 data)\n",
    "\n",
    "## Enter the Duck\n",
    "\n",
    "Using `pyarrow` and `duckdb` let's see if we get any improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "import pyarrow as pa\n",
    "from pyarrow import csv\n",
    "import pyarrow.dataset as ds\n",
    "\n",
    "def load_data_duckdb():\n",
    "    data = csv.read_csv('uber-raw-data-sep14.csv.gz', convert_options=csv.ConvertOptions(\n",
    "        include_columns=[\"Date/Time\",\"Lat\",\"Lon\"],\n",
    "        timestamp_parsers=['%m/%d/%Y %H:%M:%S']\n",
    "    )).rename_columns(['date/time', 'lat', 'lon'])\n",
    "\n",
    "    # `dataset` is for partitioning larger datasets. Can't include timestamp parsing directly though\n",
    "    # data = ds.dataset(\"uber-raw-data-sep14.csv.gz\", schema=pa.schema([\n",
    "    #     (\"Date/Time\", pa.timestamp('s')),\n",
    "    #     ('Lat', pa.float32()),\n",
    "    #     ('Lon', pa.float32())\n",
    "    # ]), format='csv')\n",
    "\n",
    "    # DuckDB can query Arrow tables, so we'll just return the table and a connection for flexible querying\n",
    "    return data, duckdb.connect(\":memory:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "121 ms Â± 4.21 ms per loop (mean Â± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "arrow_data, con = load_data_duckdb()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyarrow.Table\n",
       "date/time: timestamp[s]\n",
       "lat: double\n",
       "lon: double\n",
       "----\n",
       "date/time: [[2014-09-01 00:01:00,2014-09-01 00:01:00,2014-09-01 00:03:00,2014-09-01 00:06:00,2014-09-01 00:11:00]]\n",
       "lat: [[40.2201,40.75,40.7559,40.745,40.8145]]\n",
       "lon: [[-74.0021,-74.0027,-73.9864,-73.9889,-73.9444]]"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arrow_data[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Holy Smokes! Well that was fast and fun!\n",
    "\n",
    "`pyarrow` read the whole dataset in `120 ms`.\n",
    "That's `0.120 s` compared to `30.1 s` with `pandas`!\n",
    "\n",
    "So how much memory are `pyarrow` and `duckdb` using?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_bytes(size):\n",
    "    \"\"\"from https://stackoverflow.com/a/49361727/15685218\"\"\"\n",
    "    # 2**10 = 1024\n",
    "    power = 2**10\n",
    "    n = 0\n",
    "    power_labels = {0 : '', 1: 'kilo', 2: 'mega', 3: 'giga', 4: 'tera'}\n",
    "    while size > power:\n",
    "        size /= power\n",
    "        n += 1\n",
    "    return size, power_labels[n]+'bytes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23.53216552734375, 'megabytes')"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "format_bytes(arrow_data.nbytes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, the `pyarrow` table has roughly the same size as the full `pandas` Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0 bytes'"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "con.execute('PRAGMA database_size;')\n",
    "\"\"\"\n",
    "database_size VARCHAR, -- total block count times the block size\n",
    "block_size BIGINT,     -- database block size\n",
    "total_blocks BIGINT,   -- total blocks in the database\n",
    "used_blocks BIGINT,    -- used blocks in the database\n",
    "free_blocks BIGINT,    -- free blocks in the database\n",
    "wal_size VARCHAR,      -- write ahead log size\n",
    "memory_usage VARCHAR,  -- memory used by the database buffer manager\n",
    "memory_limit VARCHAR   -- maximum memory allowed for the database\n",
    "\"\"\"\n",
    "database_size, block_size, total_blocks, used_blocks, free_blocks, wal_size, memory_usage, memory_limit = con.fetchall()[0]\n",
    "memory_usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We haven't told `duckdb` to load anything into its own tables, so it still has no memory usage.\n",
    "Nevertheless, `duckdb` can query the `arrow_data` since it's a `pyarrow` table.\n",
    "(`duckdb` can also load directly [from csv](https://duckdb.org/docs/data/csv)).\n",
    "\n",
    "So where does that leave us on loading the full `1,000,000` row dataset?\n",
    "\n",
    "- `pandas`: `~30 s` of time and `23.5 MB`\n",
    "- `pyarrow`: `~.1 s` of time (`120 ms`) and `23.9 MB`\n",
    "\n",
    "In fairness, I tried `pandas` with the `pyarrow` engine.\n",
    "At the time of writing I can't find a fast datetime parse and `usecols` throws an error in `pyarrow` (see end of post).\n",
    "Reading the full CSV without datetime parsing is in line in terms of speed though.\n",
    "\n",
    "(also see why the best CSV [is not a CSV at all](https://pythonspeed.com/articles/pandas-read-csv-fast/) for more on this path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 33.6 s, sys: 248 ms, total: 33.9 s\n",
      "Wall time: 33.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "arrow_df = pd.read_csv(\n",
    "    \"uber-raw-data-sep14.csv.gz\",\n",
    "    engine='pyarrow',\n",
    "    names=[\n",
    "        \"date/time\",\n",
    "        \"lat\",\n",
    "        \"lon\",\n",
    "        \"CONST\"\n",
    "    ],  # specify names directly since they don't change\n",
    "    skiprows=1,  # don't read header since names specified directly\n",
    "    # usecols=[1, 2],  # doesn't load last column, constant value \"B02512\"\n",
    "    parse_dates=[\n",
    "        \"date/time\"\n",
    "    ],  # set as datetime instead of converting after the fact\n",
    "    # infer_datetime_format=True  # Unsupported for pyarrow\n",
    "    date_parser=lambda x: pd.to_datetime(x)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1028136 entries, 0 to 1028135\n",
      "Data columns (total 4 columns):\n",
      " #   Column     Non-Null Count    Dtype         \n",
      "---  ------     --------------    -----         \n",
      " 0   date/time  1028136 non-null  datetime64[ns]\n",
      " 1   lat        1028136 non-null  float64       \n",
      " 2   lon        1028136 non-null  float64       \n",
      " 3   CONST      1028136 non-null  object        \n",
      "dtypes: datetime64[ns](1), float64(2), object(1)\n",
      "memory usage: 31.4+ MB\n"
     ]
    }
   ],
   "source": [
    "arrow_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137 ms Â± 189 Âµs per loop (mean Â± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "arrow_df_no_datetime = pd.read_csv(\n",
    "    \"uber-raw-data-sep14.csv.gz\",\n",
    "    engine='pyarrow',\n",
    "    names=[\n",
    "        \"date/time\",\n",
    "        \"lat\",\n",
    "        \"lon\",\n",
    "        \"CONST\"\n",
    "    ],  # specify names directly since they don't change\n",
    "    skiprows=1,  # don't read header since names specified directly\n",
    "    # usecols=[1, 2],  # doesn't load last column, constant value \"B02512\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtration\n",
    "\n",
    "We have 3 main analysis functions to compare between `pandas` and `duckdb` for this app, laid out below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FILTER DATA FOR A SPECIFIC HOUR, CACHE\n",
    "# @st.experimental_memo\n",
    "def filterdata(df, hour_selected):\n",
    "    return df[df[\"date/time\"].dt.hour == hour_selected]\n",
    "\n",
    "\n",
    "# CALCULATE MIDPOINT FOR GIVEN SET OF DATA\n",
    "# @st.experimental_memo\n",
    "def mpoint(lat, lon):\n",
    "    return (np.average(lat), np.average(lon))\n",
    "\n",
    "\n",
    "# FILTER DATA BY HOUR\n",
    "# @st.experimental_memo\n",
    "def histdata(df, hr):\n",
    "    filtered = data[\n",
    "        (df[\"date/time\"].dt.hour >= hr) & (df[\"date/time\"].dt.hour < (hr + 1))\n",
    "    ]\n",
    "\n",
    "    hist = np.histogram(filtered[\"date/time\"].dt.minute, bins=60, range=(0, 60))[0]\n",
    "\n",
    "    return pd.DataFrame({\"minute\": range(60), \"pickups\": hist})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17.8 ms Â± 116 Âµs per loop (mean Â± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "# For fairness, we'll use the full dataframe\n",
    "filterdata(full_data, 14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "404 Âµs Â± 564 ns per loop (mean Â± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "mpoint(full_data[\"lat\"], full_data[\"lon\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/cp/ktx4zddx7q3bqctqfjykn5700000gn/T/ipykernel_82767/2026438809.py:16: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  filtered = data[\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39.5 ms Â± 275 Âµs per loop (mean Â± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "histdata(full_data, 14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How about Duckdb (with conversion back to `pandas` for fairness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def duck_filterdata(con, hour_selected):\n",
    "    return con.query(\n",
    "        f'SELECT \"date/time\", lat, lon FROM arrow_data WHERE hour(\"date/time\") = {hour_selected}'\n",
    "    ).to_df()\n",
    "\n",
    "\n",
    "def duck_mpoint(con):\n",
    "    return con.query(\"SELECT AVG(lat), AVG(lon) FROM arrow_data\").fetchone()\n",
    "\n",
    "\n",
    "def duck_histdata(con, hr):\n",
    "    hist_query = f'SELECT histogram(minute(\"date/time\")) FROM arrow_data WHERE hour(\"date/time\") >= {hr} and hour(\"date/time\") < {hr + 1}'\n",
    "    results, *_ = con.query(hist_query).fetchone()\n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.25 ms Â± 20.9 Âµs per loop (mean Â± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "duck_filterdata(con, 14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.64 ms Â± 8.87 Âµs per loop (mean Â± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "duck_mpoint(con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.64 ms Â± 26.6 Âµs per loop (mean Â± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "duck_histdata(con, 14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We got a modest improvement in `filterdata` and more than 10x speedup in `histdata`, but actually lost out to `numpy` for finding the average of 2 arrays in `mpoint`!\n",
    "\n",
    "- `filterdata`:\n",
    "  - `pandas`: 19.1 ms Â± 284 Âµs\n",
    "  - `duckdb`: 6.53 ms Â± 126 Âµs\n",
    "- `mpoint`:\n",
    "  - `numpy`: 403 Âµs Â± 5.35 Âµs\n",
    "  - `duckdb`: 1.7 ms Â± 82.6 Âµs\n",
    "- `histdata`:\n",
    "  - `pandas` + `numpy`: 40.8 ms Â± 430 Âµs\n",
    "  - `duckdb`: 2.93 ms Â± 28.4 Âµs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.9249617151607965"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "19.1 / 6.53"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.23705882352941177"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "403 / 1700"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13.924914675767916"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "40.8 / 2.93"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "It's no secret that Python is not a fast language, but there are tricks to speed it up.\n",
    "Common advice is to utilize C optimizations via `numpy` and `pandas`.\n",
    "\n",
    "Another new contender is utilizing the C++ driven `duckdb` as an in-process OLAP database manager.\n",
    "It takes some re-writing of Python code into SQL, but can play nicely with `pandas` and `pyarrow`.\n",
    "\n",
    "Speaking of Arrow, it seems to be \n",
    "\n",
    "This post explores an example `streamlit` app that utilizes some `pandas` and `numpy` functions such as `read_csv`, `average`, and DataFrame slicing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>date/time</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9/1/2014 0:01:00</td>\n",
       "      <td>1970-01-01 00:00:00.000000040</td>\n",
       "      <td>-74.0021</td>\n",
       "      <td>B02512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9/1/2014 0:01:00</td>\n",
       "      <td>1970-01-01 00:00:00.000000040</td>\n",
       "      <td>-74.0027</td>\n",
       "      <td>B02512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9/1/2014 0:03:00</td>\n",
       "      <td>1970-01-01 00:00:00.000000040</td>\n",
       "      <td>-73.9864</td>\n",
       "      <td>B02512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9/1/2014 0:06:00</td>\n",
       "      <td>1970-01-01 00:00:00.000000040</td>\n",
       "      <td>-73.9889</td>\n",
       "      <td>B02512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9/1/2014 0:11:00</td>\n",
       "      <td>1970-01-01 00:00:00.000000040</td>\n",
       "      <td>-73.9444</td>\n",
       "      <td>B02512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1028131</th>\n",
       "      <td>9/30/2014 22:57:00</td>\n",
       "      <td>1970-01-01 00:00:00.000000040</td>\n",
       "      <td>-73.9845</td>\n",
       "      <td>B02764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1028132</th>\n",
       "      <td>9/30/2014 22:57:00</td>\n",
       "      <td>1970-01-01 00:00:00.000000040</td>\n",
       "      <td>-74.1773</td>\n",
       "      <td>B02764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1028133</th>\n",
       "      <td>9/30/2014 22:58:00</td>\n",
       "      <td>1970-01-01 00:00:00.000000040</td>\n",
       "      <td>-73.9319</td>\n",
       "      <td>B02764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1028134</th>\n",
       "      <td>9/30/2014 22:58:00</td>\n",
       "      <td>1970-01-01 00:00:00.000000040</td>\n",
       "      <td>-74.0066</td>\n",
       "      <td>B02764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1028135</th>\n",
       "      <td>9/30/2014 22:58:00</td>\n",
       "      <td>1970-01-01 00:00:00.000000040</td>\n",
       "      <td>-73.9496</td>\n",
       "      <td>B02764</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1028136 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          0                     date/time      lat     lon\n",
       "0          9/1/2014 0:01:00 1970-01-01 00:00:00.000000040 -74.0021  B02512\n",
       "1          9/1/2014 0:01:00 1970-01-01 00:00:00.000000040 -74.0027  B02512\n",
       "2          9/1/2014 0:03:00 1970-01-01 00:00:00.000000040 -73.9864  B02512\n",
       "3          9/1/2014 0:06:00 1970-01-01 00:00:00.000000040 -73.9889  B02512\n",
       "4          9/1/2014 0:11:00 1970-01-01 00:00:00.000000040 -73.9444  B02512\n",
       "...                     ...                           ...      ...     ...\n",
       "1028131  9/30/2014 22:57:00 1970-01-01 00:00:00.000000040 -73.9845  B02764\n",
       "1028132  9/30/2014 22:57:00 1970-01-01 00:00:00.000000040 -74.1773  B02764\n",
       "1028133  9/30/2014 22:58:00 1970-01-01 00:00:00.000000040 -73.9319  B02764\n",
       "1028134  9/30/2014 22:58:00 1970-01-01 00:00:00.000000040 -74.0066  B02764\n",
       "1028135  9/30/2014 22:58:00 1970-01-01 00:00:00.000000040 -73.9496  B02764\n",
       "\n",
       "[1028136 rows x 4 columns]"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(\n",
    "    \"uber-raw-data-sep14.csv.gz\",\n",
    "    # nrows=100000,  # approx. 10% of data\n",
    "    engine='pyarrow',\n",
    "    names=[\n",
    "        \"date/time\",\n",
    "        \"lat\",\n",
    "        \"lon\",\n",
    "        # \"CONST\"\n",
    "    ],  # specify names directly since they don't change\n",
    "    skiprows=1,  # don't read header since names specified directly\n",
    "    # usecols=[1, 2],  # doesn't load last column, constant value \"B02512\"\n",
    "    parse_dates=[\n",
    "        \"date/time\"\n",
    "    ],  # set as datetime instead of converting after the fact\n",
    "    # # infer_datetime_format=True  # Unsupported for pyarrow\n",
    "    date_parser=lambda x: pd.to_datetime(x)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected bytes, int found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/gar/projects/demo-uber-nyc-pickups-main/blog.ipynb Cell 38'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/gar/projects/demo-uber-nyc-pickups-main/blog.ipynb#ch0000047?line=0'>1</a>\u001b[0m pd\u001b[39m.\u001b[39;49mread_csv(\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/gar/projects/demo-uber-nyc-pickups-main/blog.ipynb#ch0000047?line=1'>2</a>\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39muber-raw-data-sep14.csv.gz\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/gar/projects/demo-uber-nyc-pickups-main/blog.ipynb#ch0000047?line=2'>3</a>\u001b[0m     \u001b[39m# nrows=100000,  # approx. 10% of data\u001b[39;49;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/gar/projects/demo-uber-nyc-pickups-main/blog.ipynb#ch0000047?line=3'>4</a>\u001b[0m     engine\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mpyarrow\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/gar/projects/demo-uber-nyc-pickups-main/blog.ipynb#ch0000047?line=4'>5</a>\u001b[0m     \u001b[39m# names=[\u001b[39;49;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/gar/projects/demo-uber-nyc-pickups-main/blog.ipynb#ch0000047?line=5'>6</a>\u001b[0m     \u001b[39m#     \"date/time\",\u001b[39;49;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/gar/projects/demo-uber-nyc-pickups-main/blog.ipynb#ch0000047?line=6'>7</a>\u001b[0m     \u001b[39m#     \"lat\",\u001b[39;49;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/gar/projects/demo-uber-nyc-pickups-main/blog.ipynb#ch0000047?line=7'>8</a>\u001b[0m     \u001b[39m#     \"lon\",\u001b[39;49;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/gar/projects/demo-uber-nyc-pickups-main/blog.ipynb#ch0000047?line=8'>9</a>\u001b[0m     \u001b[39m#     \"CONST\"\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gar/projects/demo-uber-nyc-pickups-main/blog.ipynb#ch0000047?line=9'>10</a>\u001b[0m     \u001b[39m# ],  # specify names directly since they don't change\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gar/projects/demo-uber-nyc-pickups-main/blog.ipynb#ch0000047?line=10'>11</a>\u001b[0m     \u001b[39m# skiprows=1,  # don't read header since names specified directly\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gar/projects/demo-uber-nyc-pickups-main/blog.ipynb#ch0000047?line=11'>12</a>\u001b[0m     usecols\u001b[39m=\u001b[39;49m[\u001b[39m0\u001b[39;49m,\u001b[39m1\u001b[39;49m],  \u001b[39m# doesn't load last column, constant value \"B02512\"\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gar/projects/demo-uber-nyc-pickups-main/blog.ipynb#ch0000047?line=12'>13</a>\u001b[0m     \u001b[39m# parse_dates=[\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gar/projects/demo-uber-nyc-pickups-main/blog.ipynb#ch0000047?line=13'>14</a>\u001b[0m     \u001b[39m#     \"date/time\"\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gar/projects/demo-uber-nyc-pickups-main/blog.ipynb#ch0000047?line=14'>15</a>\u001b[0m     \u001b[39m# ],  # set as datetime instead of converting after the fact\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gar/projects/demo-uber-nyc-pickups-main/blog.ipynb#ch0000047?line=15'>16</a>\u001b[0m     \u001b[39m# # infer_datetime_format=True  # Unsupported for pyarrow\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gar/projects/demo-uber-nyc-pickups-main/blog.ipynb#ch0000047?line=16'>17</a>\u001b[0m     \u001b[39m# date_parser=lambda x: pd.to_datetime(x)\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gar/projects/demo-uber-nyc-pickups-main/blog.ipynb#ch0000047?line=17'>18</a>\u001b[0m )\u001b[39m.\u001b[39minfo()\n",
      "File \u001b[0;32m~/miniconda3/envs/py39/lib/python3.9/site-packages/pandas/util/_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/gar/miniconda3/envs/py39/lib/python3.9/site-packages/pandas/util/_decorators.py?line=304'>305</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[1;32m    <a href='file:///Users/gar/miniconda3/envs/py39/lib/python3.9/site-packages/pandas/util/_decorators.py?line=305'>306</a>\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    <a href='file:///Users/gar/miniconda3/envs/py39/lib/python3.9/site-packages/pandas/util/_decorators.py?line=306'>307</a>\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39marguments),\n\u001b[1;32m    <a href='file:///Users/gar/miniconda3/envs/py39/lib/python3.9/site-packages/pandas/util/_decorators.py?line=307'>308</a>\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[1;32m    <a href='file:///Users/gar/miniconda3/envs/py39/lib/python3.9/site-packages/pandas/util/_decorators.py?line=308'>309</a>\u001b[0m         stacklevel\u001b[39m=\u001b[39mstacklevel,\n\u001b[1;32m    <a href='file:///Users/gar/miniconda3/envs/py39/lib/python3.9/site-packages/pandas/util/_decorators.py?line=309'>310</a>\u001b[0m     )\n\u001b[0;32m--> <a href='file:///Users/gar/miniconda3/envs/py39/lib/python3.9/site-packages/pandas/util/_decorators.py?line=310'>311</a>\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/py39/lib/python3.9/site-packages/pandas/io/parsers/readers.py:680\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/gar/miniconda3/envs/py39/lib/python3.9/site-packages/pandas/io/parsers/readers.py?line=664'>665</a>\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    <a href='file:///Users/gar/miniconda3/envs/py39/lib/python3.9/site-packages/pandas/io/parsers/readers.py?line=665'>666</a>\u001b[0m     dialect,\n\u001b[1;32m    <a href='file:///Users/gar/miniconda3/envs/py39/lib/python3.9/site-packages/pandas/io/parsers/readers.py?line=666'>667</a>\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/gar/miniconda3/envs/py39/lib/python3.9/site-packages/pandas/io/parsers/readers.py?line=675'>676</a>\u001b[0m     defaults\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mdelimiter\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[1;32m    <a href='file:///Users/gar/miniconda3/envs/py39/lib/python3.9/site-packages/pandas/io/parsers/readers.py?line=676'>677</a>\u001b[0m )\n\u001b[1;32m    <a href='file:///Users/gar/miniconda3/envs/py39/lib/python3.9/site-packages/pandas/io/parsers/readers.py?line=677'>678</a>\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> <a href='file:///Users/gar/miniconda3/envs/py39/lib/python3.9/site-packages/pandas/io/parsers/readers.py?line=679'>680</a>\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m~/miniconda3/envs/py39/lib/python3.9/site-packages/pandas/io/parsers/readers.py:581\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/gar/miniconda3/envs/py39/lib/python3.9/site-packages/pandas/io/parsers/readers.py?line=577'>578</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n\u001b[1;32m    <a href='file:///Users/gar/miniconda3/envs/py39/lib/python3.9/site-packages/pandas/io/parsers/readers.py?line=579'>580</a>\u001b[0m \u001b[39mwith\u001b[39;00m parser:\n\u001b[0;32m--> <a href='file:///Users/gar/miniconda3/envs/py39/lib/python3.9/site-packages/pandas/io/parsers/readers.py?line=580'>581</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\u001b[39m.\u001b[39;49mread(nrows)\n",
      "File \u001b[0;32m~/miniconda3/envs/py39/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1243\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/gar/miniconda3/envs/py39/lib/python3.9/site-packages/pandas/io/parsers/readers.py?line=1240'>1241</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mengine \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpyarrow\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m   <a href='file:///Users/gar/miniconda3/envs/py39/lib/python3.9/site-packages/pandas/io/parsers/readers.py?line=1241'>1242</a>\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> <a href='file:///Users/gar/miniconda3/envs/py39/lib/python3.9/site-packages/pandas/io/parsers/readers.py?line=1242'>1243</a>\u001b[0m         df \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mread()\n\u001b[1;32m   <a href='file:///Users/gar/miniconda3/envs/py39/lib/python3.9/site-packages/pandas/io/parsers/readers.py?line=1243'>1244</a>\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[1;32m   <a href='file:///Users/gar/miniconda3/envs/py39/lib/python3.9/site-packages/pandas/io/parsers/readers.py?line=1244'>1245</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/miniconda3/envs/py39/lib/python3.9/site-packages/pandas/io/parsers/arrow_parser_wrapper.py:153\u001b[0m, in \u001b[0;36mArrowParserWrapper.read\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/gar/miniconda3/envs/py39/lib/python3.9/site-packages/pandas/io/parsers/arrow_parser_wrapper.py?line=145'>146</a>\u001b[0m pyarrow_csv \u001b[39m=\u001b[39m import_optional_dependency(\u001b[39m\"\u001b[39m\u001b[39mpyarrow.csv\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    <a href='file:///Users/gar/miniconda3/envs/py39/lib/python3.9/site-packages/pandas/io/parsers/arrow_parser_wrapper.py?line=146'>147</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_pyarrow_options()\n\u001b[1;32m    <a href='file:///Users/gar/miniconda3/envs/py39/lib/python3.9/site-packages/pandas/io/parsers/arrow_parser_wrapper.py?line=148'>149</a>\u001b[0m table \u001b[39m=\u001b[39m pyarrow_csv\u001b[39m.\u001b[39mread_csv(\n\u001b[1;32m    <a href='file:///Users/gar/miniconda3/envs/py39/lib/python3.9/site-packages/pandas/io/parsers/arrow_parser_wrapper.py?line=149'>150</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msrc,\n\u001b[1;32m    <a href='file:///Users/gar/miniconda3/envs/py39/lib/python3.9/site-packages/pandas/io/parsers/arrow_parser_wrapper.py?line=150'>151</a>\u001b[0m     read_options\u001b[39m=\u001b[39mpyarrow_csv\u001b[39m.\u001b[39mReadOptions(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mread_options),\n\u001b[1;32m    <a href='file:///Users/gar/miniconda3/envs/py39/lib/python3.9/site-packages/pandas/io/parsers/arrow_parser_wrapper.py?line=151'>152</a>\u001b[0m     parse_options\u001b[39m=\u001b[39mpyarrow_csv\u001b[39m.\u001b[39mParseOptions(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparse_options),\n\u001b[0;32m--> <a href='file:///Users/gar/miniconda3/envs/py39/lib/python3.9/site-packages/pandas/io/parsers/arrow_parser_wrapper.py?line=152'>153</a>\u001b[0m     convert_options\u001b[39m=\u001b[39mpyarrow_csv\u001b[39m.\u001b[39;49mConvertOptions(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvert_options),\n\u001b[1;32m    <a href='file:///Users/gar/miniconda3/envs/py39/lib/python3.9/site-packages/pandas/io/parsers/arrow_parser_wrapper.py?line=153'>154</a>\u001b[0m )\n\u001b[1;32m    <a href='file:///Users/gar/miniconda3/envs/py39/lib/python3.9/site-packages/pandas/io/parsers/arrow_parser_wrapper.py?line=155'>156</a>\u001b[0m frame \u001b[39m=\u001b[39m table\u001b[39m.\u001b[39mto_pandas()\n\u001b[1;32m    <a href='file:///Users/gar/miniconda3/envs/py39/lib/python3.9/site-packages/pandas/io/parsers/arrow_parser_wrapper.py?line=156'>157</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_finalize_output(frame)\n",
      "File \u001b[0;32m~/miniconda3/envs/py39/lib/python3.9/site-packages/pyarrow/_csv.pyx:580\u001b[0m, in \u001b[0;36mpyarrow._csv.ConvertOptions.__init__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/py39/lib/python3.9/site-packages/pyarrow/_csv.pyx:734\u001b[0m, in \u001b[0;36mpyarrow._csv.ConvertOptions.include_columns.__set__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mstringsource:15\u001b[0m, in \u001b[0;36mstring.from_py.__pyx_convert_string_from_py_std__in_string\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: expected bytes, int found"
     ]
    }
   ],
   "source": [
    "pd.read_csv(\n",
    "    \"uber-raw-data-sep14.csv.gz\",\n",
    "    # nrows=100000,  # approx. 10% of data\n",
    "    engine='pyarrow',\n",
    "    # names=[\n",
    "    #     \"date/time\",\n",
    "    #     \"lat\",\n",
    "    #     \"lon\",\n",
    "    #     \"CONST\"\n",
    "    # ],  # specify names directly since they don't change\n",
    "    # skiprows=1,  # don't read header since names specified directly\n",
    "    usecols=[0,1],  # doesn't load last column, constant value \"B02512\"\n",
    "    # parse_dates=[\n",
    "    #     \"date/time\"\n",
    "    # ],  # set as datetime instead of converting after the fact\n",
    "    # # infer_datetime_format=True  # Unsupported for pyarrow\n",
    "    # date_parser=lambda x: pd.to_datetime(x)\n",
    ").info()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5cce4339234dc84068df7ee439717747529a76a445a8b3e1bf5cb48e928293c5"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 ('py39')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
