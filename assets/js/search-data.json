{
  
    
        "post0": {
            "title": "Scanning URLs from Images",
            "content": "Scanning QR codes for humans: URLs . The idea for this popped up when talking to my mom about how QR codes work. The simple usecase for QR codes is the same as a URL: get the user to a certain website. . So why can&#39;t we just scan a real URL with our cameras...? (spoiler: many modern android and ios devices can, but that&#39;s a short end to the story) . I&#39;m all for QR codes being used for more things, but URLs aren&#39;t going away any time soon for a few reasons: . Not everybody knows how to make a QR code (hint: it takes a few lines of Python if you use the python-qrcode library) | URLs often contain the company name so people can remember them (qr codes can contain logos in some cases) | URLs are a single line (most of the time) rather than taking up a whole square of printing / advertising space | . I also wanted to give a live Streamlit app a shot after using it for some AI demos with students. So was born the Streamlit URL Scanner! . In this notebook I&#39;ll breakdown the basic mechanisms behind the app, see the full source code on github . Python Dependencies . There are 3 goals for this app: . Allow user to upload images containing URLs to the web app | Run high-accuracy Optical Character Recognition (OCR) on the uploaded image | Provide any URLs from the extracted text to the user for easy clicking | . I went with the libraries that I thought would give the fastest and most successful development experience. . Python Web App . streamlit: Python rapid development web app framework Provides a file upload component out of the box with st.file_uploader | Simple Cloud deployment with secure secrets for OCR component | . | . Contenders . fastapi: Providing a route to OCR component as a service Asynchronous by default is nice for handling distributed transactions to OCR task | . | django: Overbaked for usecase This is a proof of concept tool, not a full-stack user-oriented website | . | flask / bottle: Have ways of being asynchronous, but synchronous by default | . | . All of these other options would require a frontend app or integrating some JS library or other index.html + JS + CSS combination . OCR . AWS Rekognition: Trained for text detection on real world images Limited to 100 words detected | Accessed with the boto3 library | . | . Contenders . AWS Textract: More tuned for documents than real world | Tesseract: Still a good OCR, but also focused on documents Can be self-hosted, not paid per transaction | . | . URL Extraction . urlextract: I didn&#39;t want to write a regex for URLs when there&#39;s a small library without any known issues | . Glue Code . PIL / Pillow: Python Imaging Library for handling user uploaded images and resizing if needed | pydantic: Typed Settings management with loading from CLI, Environment, Secrets | . import io import json import boto3 import streamlit as st from PIL import Image, ImageDraw, ImageOps from pydantic import BaseSettings from urlextract import URLExtract . OCR + Extractor Setup . Pydantic&#39;s BaseSettings Class allows us to read in settings for connecting to AWS account. This can be used with Docker secrets, but this app is deployed to Streamlit cloud. . boto3 lets us establish a client to the Rekognition service. . URL Extract requires some initialization to recognize all domain names. . class Settings(BaseSettings): &quot;&quot;&quot;Handles fetching configuration from environment variables and secrets. Type-hinting for config as a bonus&quot;&quot;&quot; aws_access_key_id: str aws_secret_access_key: str aws_region: str settings = Settings() rekog_client = boto3.client( &quot;rekognition&quot;, region_name=settings.aws_region, aws_access_key_id=settings.aws_access_key_id, aws_secret_access_key=settings.aws_secret_access_key, ) extractor = URLExtract() . 2022-01-27 18:44:24.510 INFO filelock: Lock 139763555315184 acquired on /home/gar/miniconda3/lib/python3.8/site-packages/urlextract/data/tlds-alpha-by-domain.txt.lock 2022-01-27 18:44:24.555 INFO filelock: Lock 139763555315184 released on /home/gar/miniconda3/lib/python3.8/site-packages/urlextract/data/tlds-alpha-by-domain.txt.lock . Detecting Text in an Image . AWS Rekognition can receive either a path to an S3 object or raw image bytes. For this app I went with passing just the image bytes, so a helper function to compress larger images was needed. We&#39;ll ignore the streamlit specific alert message that this is happening for this demo. (The S3 version isn&#39;t much more complicated, and is beneficial for more general OCR apps) . Another small helper for passing the correct parameters to boto3 will wrap up this section. . Pillow will do our image handling in the app, so we&#39;ll use it for showing a demo detection in the following code cells . def compress_pil_image(image: Image, limit=(5 * (2 ** 20))) -&gt; bytes: &quot;&quot;&quot;Takes a Pillow image and returns byte values of the image saved as png. Reduces dimensions of image if it is larger than provided limit. Args: image (Image): Image to get the bytes for limit (int, optional): Maximum number of bytes. Defaults to 5mb (5 * (2 ** 20)). Returns: bytes: image saved as PNG bytes object &quot;&quot;&quot; image_bytes = io.BytesIO() image.save(image_bytes, &quot;PNG&quot;) output = image_bytes.getvalue() limit_to_bytes_ratio = limit / len(output) if limit_to_bytes_ratio &gt;= 1.0: return output else: # st.warning(f&quot;Resizing by ratio: {limit_to_bytes_ratio}&quot;) width, height = image.size new_width = int(width * limit_to_bytes_ratio) new_height = int(height * limit_to_bytes_ratio) new_image = image.resize((new_width, new_height), Image.ANTIALIAS) return compress_pil_image(new_image, limit) def rekog_detect_by_bytes(image_bytes: bytes) -&gt; dict: &quot;&quot;&quot;Takes an array of bytes representing jpg / png image. Tries to return response from AWS Rekognition detect_text API on the image bytes See docs for more: https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/rekognition.html#Rekognition.Client.detect_text # noqa: E501 Args: image_bytes (bytes): Image to run detection on (less than 5 mb) Returns: dict: List of text detections, geometry of the detections, and metadata &quot;&quot;&quot; response = rekog_client.detect_text(Image={&quot;Bytes&quot;: image_bytes}) return response . demo_image = Image.open(&#39;test_images/sample_urls.jpg&#39;) demo_image . image_bytes = compress_pil_image(demo_image) rekognotion_response = rekog_detect_by_bytes(image_bytes) rekognotion_response . {&#39;TextDetections&#39;: [{&#39;DetectedText&#39;: &#39;Lorem ipsum...&#39;, &#39;Type&#39;: &#39;LINE&#39;, &#39;Id&#39;: 0, &#39;Confidence&#39;: 83.6632080078125, &#39;Geometry&#39;: {&#39;BoundingBox&#39;: {&#39;Width&#39;: 0.198333278298378, &#39;Height&#39;: 0.05164804682135582, &#39;Left&#39;: 0.4017779529094696, &#39;Top&#39;: 0.3004961609840393}, &#39;Polygon&#39;: [{&#39;X&#39;: 0.4021288752555847, &#39;Y&#39;: 0.3004961609840393}, {&#39;X&#39;: 0.6001112461090088, &#39;Y&#39;: 0.3018783628940582}, {&#39;X&#39;: 0.5997602939605713, &#39;Y&#39;: 0.3521442115306854}, {&#39;X&#39;: 0.4017779529094696, &#39;Y&#39;: 0.3507620096206665}]}}, {&#39;DetectedText&#39;: &#39;google.com&#39;, &#39;Type&#39;: &#39;LINE&#39;, &#39;Id&#39;: 1, &#39;Confidence&#39;: 99.61248779296875, &#39;Geometry&#39;: {&#39;BoundingBox&#39;: {&#39;Width&#39;: 0.142822265625, &#39;Height&#39;: 0.0458984375, &#39;Left&#39;: 0.428466796875, &#39;Top&#39;: 0.3544921875}, &#39;Polygon&#39;: [{&#39;X&#39;: 0.428466796875, &#39;Y&#39;: 0.3544921875}, {&#39;X&#39;: 0.5712890625, &#39;Y&#39;: 0.3544921875}, {&#39;X&#39;: 0.5712890625, &#39;Y&#39;: 0.400390625}, {&#39;X&#39;: 0.428466796875, &#39;Y&#39;: 0.400390625}]}}, {&#39;DetectedText&#39;: &#39;https://streamlit.io/&#39;, &#39;Type&#39;: &#39;LINE&#39;, &#39;Id&#39;: 2, &#39;Confidence&#39;: 97.68013763427734, &#39;Geometry&#39;: {&#39;BoundingBox&#39;: {&#39;Width&#39;: 0.2527962923049927, &#39;Height&#39;: 0.051752522587776184, &#39;Left&#39;: 0.37402498722076416, &#39;Top&#39;: 0.45518985390663147}, &#39;Polygon&#39;: [{&#39;X&#39;: 0.37402498722076416, &#39;Y&#39;: 0.455630898475647}, {&#39;X&#39;: 0.6267316937446594, &#39;Y&#39;: 0.45518985390663147}, {&#39;X&#39;: 0.6268212795257568, &#39;Y&#39;: 0.506501317024231}, {&#39;X&#39;: 0.3741145431995392, &#39;Y&#39;: 0.5069423913955688}]}}, {&#39;DetectedText&#39;: &#39;Lorem&#39;, &#39;Type&#39;: &#39;WORD&#39;, &#39;Id&#39;: 3, &#39;ParentId&#39;: 0, &#39;Confidence&#39;: 99.94923400878906, &#39;Geometry&#39;: {&#39;BoundingBox&#39;: {&#39;Width&#39;: 0.07525634765625, &#39;Height&#39;: 0.0341796875, &#39;Left&#39;: 0.402099609375, &#39;Top&#39;: 0.3046875}, &#39;Polygon&#39;: [{&#39;X&#39;: 0.402099609375, &#39;Y&#39;: 0.3046875}, {&#39;X&#39;: 0.47735595703125, &#39;Y&#39;: 0.3046875}, {&#39;X&#39;: 0.47735595703125, &#39;Y&#39;: 0.3388671875}, {&#39;X&#39;: 0.402099609375, &#39;Y&#39;: 0.3388671875}]}}, {&#39;DetectedText&#39;: &#39;ipsum...&#39;, &#39;Type&#39;: &#39;WORD&#39;, &#39;Id&#39;: 4, &#39;ParentId&#39;: 0, &#39;Confidence&#39;: 67.37718200683594, &#39;Geometry&#39;: {&#39;BoundingBox&#39;: {&#39;Width&#39;: 0.11253990232944489, &#39;Height&#39;: 0.05104871839284897, &#39;Left&#39;: 0.48755744099617004, &#39;Top&#39;: 0.30109521746635437}, &#39;Polygon&#39;: [{&#39;X&#39;: 0.4879346489906311, &#39;Y&#39;: 0.30109521746635437}, {&#39;X&#39;: 0.6000973582267761, &#39;Y&#39;: 0.30386465787887573}, {&#39;X&#39;: 0.5997201800346375, &#39;Y&#39;: 0.35214391350746155}, {&#39;X&#39;: 0.48755744099617004, &#39;Y&#39;: 0.3493745028972626}]}}, {&#39;DetectedText&#39;: &#39;google.com&#39;, &#39;Type&#39;: &#39;WORD&#39;, &#39;Id&#39;: 5, &#39;ParentId&#39;: 1, &#39;Confidence&#39;: 99.61248779296875, &#39;Geometry&#39;: {&#39;BoundingBox&#39;: {&#39;Width&#39;: 0.142822265625, &#39;Height&#39;: 0.0458984375, &#39;Left&#39;: 0.428466796875, &#39;Top&#39;: 0.3544921875}, &#39;Polygon&#39;: [{&#39;X&#39;: 0.428466796875, &#39;Y&#39;: 0.3544921875}, {&#39;X&#39;: 0.5712890625, &#39;Y&#39;: 0.3544921875}, {&#39;X&#39;: 0.5712890625, &#39;Y&#39;: 0.400390625}, {&#39;X&#39;: 0.428466796875, &#39;Y&#39;: 0.400390625}]}}, {&#39;DetectedText&#39;: &#39;https://streamlit.io/&#39;, &#39;Type&#39;: &#39;WORD&#39;, &#39;Id&#39;: 6, &#39;ParentId&#39;: 2, &#39;Confidence&#39;: 97.68013763427734, &#39;Geometry&#39;: {&#39;BoundingBox&#39;: {&#39;Width&#39;: 0.2527649700641632, &#39;Height&#39;: 0.051752522587776184, &#39;Left&#39;: 0.3740406334400177, &#39;Top&#39;: 0.45518985390663147}, &#39;Polygon&#39;: [{&#39;X&#39;: 0.3740406334400177, &#39;Y&#39;: 0.4563566744327545}, {&#39;X&#39;: 0.6267316937446594, &#39;Y&#39;: 0.45518985390663147}, {&#39;X&#39;: 0.6268056035041809, &#39;Y&#39;: 0.5057755708694458}, {&#39;X&#39;: 0.3741145431995392, &#39;Y&#39;: 0.5069423913955688}]}}], &#39;TextModelVersion&#39;: &#39;3.0&#39;, &#39;ResponseMetadata&#39;: {&#39;RequestId&#39;: &#39;6ee34f23-945f-45ea-9fe4-439580da7ff2&#39;, &#39;HTTPStatusCode&#39;: 200, &#39;HTTPHeaders&#39;: {&#39;x-amzn-requestid&#39;: &#39;6ee34f23-945f-45ea-9fe4-439580da7ff2&#39;, &#39;content-type&#39;: &#39;application/x-amz-json-1.1&#39;, &#39;content-length&#39;: &#39;2883&#39;, &#39;date&#39;: &#39;Fri, 28 Jan 2022 02:56:30 GMT&#39;}, &#39;RetryAttempts&#39;: 0}} . Extracting URLs from Text . If you&#39;re not familiar with APIs or bounding boxes the above output might be a bit of a mess. That&#39;s alright, we&#39;re here to work through it. . Rekognition&#39;s text detection returns a List of &quot;Text Detection&quot; records. Each of these &quot;Text Detections&quot; has a few features, but the most important to our purpose is &quot;Detected Text.&quot; . If we&#39;re really just interested in the text, we can use a list comprehension to get the detections and pass them to the URL extractor . detected_text = [detection[&#39;DetectedText&#39;] for detection in rekognotion_response[&#39;TextDetections&#39;] if detection[&quot;Type&quot;] == &quot;LINE&quot;] extracted_urls = extractor.find_urls(&quot; &quot;.join(detected_text)) extracted_urls . [&#39;google.com&#39;, &#39;https://streamlit.io/&#39;] . Streamlit aspect . Streamlit provides the frontend components for uploading and viewing images and links. (And giving a semblance of user experience) . It&#39;s hard to demo these aspects in a notebook, but here are the streamlit snippets and use cases in the app. . # Header and Description st.title(&quot;URL Scan :computer:&quot;) st.header( &quot;Never type a URL from real life again! &quot; &quot;Take a picture with a URL in it and we&#39;ll scan any links so you can click them!&quot; ) st.subheader(&quot;(Or upload an image you already have on your device)&quot;) # Retrieve image from camera or upload camera_bytes = st.camera_input(&quot;Take a picture&quot;) uploaded_bytes = st.file_uploader( &quot;Upload an image&quot;, type=[&quot;png&quot;, &quot;jpg&quot;, &quot;jpeg&quot;], ) # Context manager to give better loading experience with st.spinner(&quot;Loading Image Bytes&quot;): # Compress pil image pass # Provide visual alerts to the user st.success( f&quot;Found {len(extracted_urls)} URLs!&quot; ) # Allow downloading the detected text / urls st.download_button( label=&quot;Download extracted url list&quot;, data=&#39; n&#39;.join(extracted_urls), file_name=&quot;extracted_urls.txt&quot;, mime=&quot;text&quot;, ) # Display the raw and detected images st.image( demo_image, use_column_width=True, ) . Testing and deployment . Docker is used to help smooth environments between windows / linux / mac. Docker-compose is used to open up to future extensions with other backend apps. . Linting, Static Checking, and Testing are handled locally before deployment. . E2E testing consists of Selenium visual baseline testing against locally deployed app in docker-compose . Deployment would be straightforward with docker-compose, but Streamlit cloud provides plenty of resources for this use case. Not having to write a CI/CD pipeline is fine by me. . Downsides are the need to deploy secrets manually to streamlit and it requires setting up a seperate app deployment if a staging / UAT environment is desired. . BONUS: Painting detections . The url extraction code used above isn&#39;t the same process as used in the app. I think the bounding box aspect of text detection is engaging for users to understand the OCR component, so I include a copy of their image with the bounding boxes painted on. . We get all of the location data in the &quot;Text Detections&quot; from Rekognition, but we have to do a bit of conversion from their format to draw them with Pillow&#39;s ImageDraw. In this case we&#39;re converting from a format that provides the Width, Height, Left-side X coordinate, and Top-side Y coordinate in percentage of the image size. . Our goal is to use some arithmetic to get the (X,Y) coordinates of the top-left corner of our bounding box and the bottom-right corner in pixels. (If you haven&#39;t worked with bounding boxes, there&#39;s even more possible formats...) . image_w, image_h = demo_image.size painted_image = demo_image.copy() canvas = ImageDraw.Draw(painted_image) for detection in rekognotion_response[&quot;TextDetections&quot;]: if detection[&quot;Type&quot;] == &quot;LINE&quot;: text = detection[&quot;DetectedText&quot;] aws_bbox = detection[&quot;Geometry&quot;][&quot;BoundingBox&quot;] top_left_x = aws_bbox[&quot;Left&quot;] * image_w top_left_y = aws_bbox[&quot;Top&quot;] * image_h box_width = aws_bbox[&quot;Width&quot;] * image_w box_height = aws_bbox[&quot;Height&quot;] * image_h bot_right_x = top_left_x + box_width bot_right_y = top_left_y + box_height canvas.rectangle( (top_left_x, top_left_y, bot_right_x, bot_right_y), outline=&quot;Red&quot;, width=3, ) painted_image .",
            "url": "https://gerardrbentley.github.io/streamlit/python/aws/2022/01/24/streamlit-url-scan.html",
            "relUrl": "/streamlit/python/aws/2022/01/24/streamlit-url-scan.html",
            "date": " • Jan 24, 2022"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "I’m a software developer interested in web-based apps, ethical / explainable AI, and bringing coding to others. . I started programming at Hotchkiss, back in 2013. I took the one and only Java class then left off until college. . I graduated from Pomona College in 2019 with a BA in Computer Science, then stuck around for the summer and fall as part of a post-bacc research year. Working with professor Joe Osborn we authored “The Videogame Affordances Corpus” at EXAG 2019, which was the first paper for our ‘Formal Analysis of Interactive Media (FAIM) Lab’. My work over that year focused mainly on testing Pytorch models to analyze in-game screenshots from a collection of classic action and adventure games. . Through the pandemic I took a contract role for an Edu-tech startup Edlyft (started by some fellow Hotchkiss alums!) . In January of 2021 I started a job with Cascade Financial Services doing Python automation and working on some legacy projects. . The only machine learning I work on there focuses on mortgage returns and already has established models and usecases. So I started part-time teaching AI and Python topics with AI Camp to get back to education and ML. .",
          "url": "https://gerardrbentley.github.io/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://gerardrbentley.github.io/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}