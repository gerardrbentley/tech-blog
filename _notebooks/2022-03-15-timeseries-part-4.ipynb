{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"Time Series Data Part 4: A Full Stack Use Case\"\n",
    "> Data Analysis meets Data Science meets Data Engineering meets Data Operations\n",
    "\n",
    "- toc: true \n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: [streamlit, darts, python, timeseries, intermediate]\n",
    "- image: images/2022-03-15-14-20-49.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Series Data Part 4: A Full Stack Use Case\n",
    "\n",
    "## Roommate Spending Ledger Visualization\n",
    "\n",
    "[![Open in Streamlit](https://static.streamlit.io/badges/streamlit_badge_black_white.svg)](https://share.streamlit.io/gerardrbentley/roommate-ledger/main/app.py)\n",
    "\n",
    "Using Pandas + Plotly + SQLite to show a full stack use case of Time Series Data.\n",
    "\n",
    "Analyze spending over time per person (could be adapted to categories / tags / etc).\n",
    "\n",
    "See: [Github Repo](https://github.com/gerardrbentley/roommate-ledger)\n",
    "\n",
    "## Idea\n",
    "\n",
    "One time series that any financially independent person should pay attention to is their spending over time.\n",
    "\n",
    "Tracking your spending gets complicated with roommates and budgets for different categories.\n",
    "It also complicates your understanding of your data with a glance, which is where charts and graphs can help.\n",
    "\n",
    "There are many personal budgeting and even group budgeting apps, but I wanted to make the simplest possible and stick to the Data and Visualizations as an MVP.\n",
    "\n",
    "One way to get this data is from a CSV export from a bank account or credit card company.\n",
    "In [Part 3]({% post_url 2022-03-11-timeseries-part-3 %}) is an app that uses this method on general time series data.\n",
    "Upload a CSV with some column for time stamps and some column to forecast and tune your own prediction model!\n",
    "\n",
    "The main drawbacks to this paradigm:\n",
    "\n",
    "- Can't share data between people / sessions\n",
    "- Can't persist data\n",
    "- Can't incrementally add data\n",
    "- Can't update data\n",
    "\n",
    "Another way is the CRUD paradigm explored in my [Streamlit Full Stack Post]({% post_url 2022-02-10-streamlit-fullstack %}).\n",
    "With this method we'll be able to operate on individual data points and let our friends / roommates add to it!\n",
    "\n",
    "(Of course the CSV paradigm could be blended with this)\n",
    "\n",
    "Now for each aspect of the app, from Backend to Front\n",
    "\n",
    "## DevOps\n",
    "\n",
    "There's not much real DataOps in this project since the data is self-contained.\n",
    "\n",
    "That said, there are some DevOps aspects that are important in the time series world:\n",
    "\n",
    "- Deployment: having a webserver accessible to multiple users\n",
    "- Integration: how to get updated code into the deployment(s)\n",
    "\n",
    "Leaning on Streamlit Cloud sharing checks both of these boxes with ease.\n",
    "\n",
    "By including a `requirements.txt` and specifying a python version for the image, we get a free CI/CD pipeline from any push to a github branch (more providers to come).\n",
    "It'll provide us with an Ubuntu-like container that installs all requirements and tries to perform `streamlit run streamlit_app.py`, yielding a live webserver accessible to the public for public repos!\n",
    "\n",
    "## Backend\n",
    "\n",
    "The Data Engineering aspect involves a bit of data design and a bit of service writing.\n",
    "\n",
    "I decided the minimum data to track expenses are:\n",
    "\n",
    "- `purchased_date`\n",
    "  - The day on which the purchase was made\n",
    "- `purchased_by`\n",
    "  - The name of the person who made the purchase\n",
    "- `price_in_cents`\n",
    "  - Price of the purchase. Tracked in cents to avoid [floating point perils](https://www.lahey.com/float.htm)\n",
    "\n",
    "Relying on SQLite, we'll have to represent the date as a string, but `pandas` will help us transform it to a date / datetime object.\n",
    "A table creation routine with SQLite for this might look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "from typing import Optional\n",
    "\n",
    "def get_connection(connection_string: str = \":memory:\") -> sqlite3.Connection:\n",
    "    \"\"\"Make a connection object to sqlite3 with key-value Rows as outputs\n",
    "    - https://stackoverflow.com/questions/48218065/programmingerror-sqlite-objects-created-in-a-thread-can-only-be-used-in-that-sa\n",
    "    \"\"\"\n",
    "    connection = sqlite3.connect(connection_string)\n",
    "    connection.row_factory = sqlite3.Row\n",
    "    return connection\n",
    "\n",
    "def execute_query(\n",
    "    connection: sqlite3.Connection, query: str, args: Optional[dict] = None\n",
    ") -> list:\n",
    "    \"\"\"Given sqlite3.Connection and a string query (and optionally necessary query args as a dict),\n",
    "    Attempt to execute query with cursor, commit transaction, and return fetched rows\"\"\"\n",
    "    cur = connection.cursor()\n",
    "    if args is not None:\n",
    "        cur.execute(query, args)\n",
    "    else:\n",
    "        cur.execute(query)\n",
    "    connection.commit()\n",
    "    results = cur.fetchall()\n",
    "    cur.close()\n",
    "    return results\n",
    "\n",
    "def create_expenses_table(connection: sqlite3.Connection) -> None:\n",
    "    \"\"\"Create Expenses Table in the database if it doesn't already exist\"\"\"\n",
    "    init_expenses_query = f\"\"\"CREATE TABLE IF NOT EXISTS expenses(\n",
    "   purchased_date VARCHAR(10) NOT NULL,\n",
    "   purchased_by VARCHAR(120) NOT NULL,\n",
    "   price_in_cents INT NOT NULL);\"\"\"\n",
    "    execute_query(connection, init_expenses_query)\n",
    "\n",
    "connection = get_connection()\n",
    "create_expenses_table(connection)\n",
    "info_results = execute_query(connection, \"SELECT name, type, sql FROM sqlite_schema WHERE name = 'expenses'\")\n",
    "info = info_results[0]\n",
    "info['name'], info['type'], info['sql']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also get a free autoincrementing `rowid` from SQLite, which will differentiate any purchases by the same person on the same day for the same amount!\n",
    "\n",
    "### Python Object Model\n",
    "\n",
    "That's all well and good for a DBA, but what about the Python glue?\n",
    "\n",
    "Using `pydantic` / `dataclasses` is my preferred way to make Python classes that represent database objects or API responses.\n",
    "Splitting the Model into a child class for the internal application usage and parent class for database syncing is one way to handle auto-created id's and optional vs. required arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class BaseExpense(BaseModel):\n",
    "    price_in_cents: int\n",
    "    purchased_date: date\n",
    "    purchased_by: str\n",
    "\n",
    "class Expense(BaseExpense):\n",
    "    rowid: int\n",
    "\n",
    "Expense(rowid=1, price_in_cents=100, purchased_date=date(2022, 3, 15), purchased_by='gar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seeding\n",
    "\n",
    "To get some values for playing around with and demonstrating Create / Update, here's a snippet of seeding the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def seed_expenses_table(connection: sqlite3.Connection) -> None:\n",
    "    \"\"\"Insert sample Expense rows into the database\"\"\"\n",
    "    for i in range(200):\n",
    "        seed_expense = Expense(\n",
    "            rowid=i,\n",
    "            purchased_date=date(\n",
    "                random.randint(2020, 2022), random.randint(1, 12), random.randint(1, 28)\n",
    "            ).strftime(\"%Y-%m-%d\"),\n",
    "            purchased_by=random.choice([\"Alice\", \"Bob\", \"Chuck\"]),\n",
    "            price_in_cents=random.randint(50, 100_00),\n",
    "        )\n",
    "        seed_expense_query = f\"\"\"REPLACE into expenses(rowid, purchased_date, purchased_by, price_in_cents)\n",
    "        VALUES(:rowid, :purchased_date, :purchased_by, :price_in_cents);\"\"\"\n",
    "        execute_query(connection, seed_expense_query, seed_expense.dict())\n",
    "\n",
    "seed_expenses_table(connection)\n",
    "print('Seeded 200 rows')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "200 times we'll create and save an Expense object with a hardcoded id and some random values for date, purchaser, and price.\n",
    "(Note the randomized days max out at 28 to avoid headaches with february being short. There's probably a builtin to help with random days, maybe just timedelta with random amount is easier)\n",
    "\n",
    "Using `kwarg` placeholders of the form `:keyname` lets us pass the dictionary / JSON representation of our Python object instead of specifying each invidual field in the correct order.\n",
    "\n",
    "The rest of the CRUD operations follow a similar pattern.\n",
    "Reading is the only hacky function to allow filtering / querying at database level before pulling **ALL** records into memory.\n",
    "\n",
    "## Reshaping the Data\n",
    "\n",
    "The Data Science aspect of this involves massaging the data into something useful to display.\n",
    "\n",
    "The data as it stands is not actually the well formed time series you might have thought.\n",
    "\n",
    "Sure the date stamps are all real, but what value do we read from them?\n",
    "The goal is to track spending (`price_in_cents` in the database).\n",
    "\n",
    "But what if we have multiple purchases on the same day?\n",
    "Then we might start treating all the purchases on a given day as stochastic samples and that is not our use case. (But that might fit your use case if you are trying to model behaviour based off of **many** people's purchases)\n",
    "\n",
    "### Enter the Pandas\n",
    "\n",
    "Utilizing Pydantic to parse / validate our database data then dumping as a list of dictionaries for Pandas to handle gets us a dataframe with all the expenses we want to see.\n",
    "Passing a `start_date`, `end_date`, and `selections` will limit the data to certain time range and `purchased_by` users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#collapse-hide\n",
    "from typing import List\n",
    "\n",
    "class ExpenseService:\n",
    "    \"\"\"Namespace for Database Related Expense Operations\"\"\"\n",
    "\n",
    "    def list_all_purchasers(connection: sqlite3.Connection) -> List[str]:\n",
    "        select_purchasers = \"SELECT DISTINCT purchased_by FROM expenses\"\n",
    "        expense_rows = execute_query(connection, select_purchasers)\n",
    "        return [x[\"purchased_by\"] for x in expense_rows]\n",
    "\n",
    "    def list_all_expenses(\n",
    "        connection: sqlite3.Connection,\n",
    "        start_date: Optional[date] = None,\n",
    "        end_date: Optional[date] = None,\n",
    "        selections: Optional[list[str]] = None,\n",
    "    ) -> List[sqlite3.Row]:\n",
    "        \"\"\"Returns rows from all expenses. Ordered in reverse creation order\"\"\"\n",
    "        select = (\n",
    "            \"SELECT rowid, purchased_date, purchased_by, price_in_cents FROM expenses\"\n",
    "        )\n",
    "        where = \"\"\n",
    "        do_and = True\n",
    "        kwargs = {}\n",
    "        if any(x is not None for x in (start_date, end_date, selections)):\n",
    "            where = \"WHERE\"\n",
    "        if start_date is not None:\n",
    "            where += \" purchased_date >= :start_date\"\n",
    "            kwargs[\"start_date\"] = start_date\n",
    "            do_and = True\n",
    "        if end_date is not None:\n",
    "            if do_and:\n",
    "                where += \" and\"\n",
    "            where += \" purchased_date <= :end_date\"\n",
    "            kwargs[\"end_date\"] = end_date\n",
    "            do_and = True\n",
    "        if selections is not None:\n",
    "            if do_and:\n",
    "                where += \" and\"\n",
    "            selection_map = {str(i): x for i, x in enumerate(selections)}\n",
    "            where += (\n",
    "                f\" purchased_by IN ({','.join(':' + x for x in selection_map.keys())})\"\n",
    "            )\n",
    "            kwargs.update(selection_map)\n",
    "\n",
    "        order_by = \"ORDER BY purchased_date DESC;\"\n",
    "        query = \" \".join((select, where, order_by))\n",
    "        expense_rows = execute_query(connection, query, kwargs)\n",
    "        return expense_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "expense_rows = ExpenseService.list_all_expenses(\n",
    "    connection, date(1900, 1, 1), date(2023, 1, 1), ['Alice', 'Bob', 'Chuck']\n",
    ")\n",
    "expenses = [Expense(**row) for row in expense_rows]\n",
    "raw_df = pd.DataFrame([x.dict() for x in expenses])\n",
    "raw_df.iloc[:3]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this analysis I mainly care about total purchase amount per day per person.\n",
    "This means the rowid doesn't really matter to me as a unique identifier, so let's drop it.\n",
    "\n",
    "(This indexing selection will also re-order your columns if you do or do not want that)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = raw_df[[\"purchased_date\", \"purchased_by\", \"price_in_cents\"]]\n",
    "df.iloc[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To handle the summation of each person's purchase per day, pandas `pivot_table` provides us the grouping and sum in one function call.\n",
    "\n",
    "This will get us roughly columnar shaped data for each person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_df = df.pivot_table(\n",
    "    index=\"purchased_date\", columns=\"purchased_by\", aggfunc=\"sum\", fill_value=0\n",
    ")\n",
    "pivot_df.iloc[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking more like a time series!\n",
    "\n",
    "`pivot_table` had the minor side effect of adding a multi index, which can be popped off if not relevant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_df.columns = pivot_df.columns.droplevel(0)\n",
    "pivot_df.iloc[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Side note:\n",
    "I also added a feature where \"All\" is a valid selection in addition to all `purchased_by` users.\n",
    "\n",
    "The \"All\" spending per day is the sum of each row!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_df['All'] = pivot_df.sum(axis=1)\n",
    "pivot_df[(pivot_df.Alice > 0) & (pivot_df.Bob > 0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To fill in date gaps (make the time series have a well defined period of one day), one way is to build your own range of dates and then reindex the time series dataframe with the full range of dates.\n",
    "\n",
    "Grabbing the min and max of the current index gets the start and end points for the range.\n",
    "Filling with 0 is fine by me since there were no purchases on those days\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_date = pivot_df.index.min()\n",
    "max_date = pivot_df.index.max()\n",
    "all_dates = pd.date_range(min_date, max_date, freq=\"D\", name=\"purchased_date\")\n",
    "pivot_df = pivot_df.reindex(all_dates, fill_value=0)\n",
    "pivot_df.iloc[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the cumulative spend up to each point in time, pandas provides `cumsum()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cumulative = pivot_df.cumsum()\n",
    "cumulative.iloc[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And to analyze percentage contributed to the whole group's cumulative spending we can divide by the sum of each cumulative row.\n",
    "\n",
    "(We included the \"All\" summation already, so this case is actually slightly over-complicated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentages = (\n",
    "    cumulative[cumulative.columns.drop(\"All\", errors=\"ignore\")]\n",
    "    .divide(cumulative.sum(axis=1), axis=0)\n",
    "    .multiply(100)\n",
    ")\n",
    "percentages.iloc[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grabbing the totals of each spender might be a nice metric to display.\n",
    "\n",
    "This could also be grabbed from the end of the cumulative data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "totals = pivot_df.sum()\n",
    "totals.index.name = \"purchased_by\"\n",
    "totals.name = \"value\"\n",
    "totals = totals.div(100).reset_index()\n",
    "totals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas also provides a convenient `rolling()` function for applying tranformations on moving windows.\n",
    "\n",
    "In this case let's get the cumulative spending per 7 days per person.\n",
    "\n",
    "Notice that the value will stay the same on days when the person made `$0.00` of purchases, since `x + 0 = x`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rolling_df = pivot_df.rolling(7, min_periods=1).sum()\n",
    "rolling_df.iloc[:8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't have to sum the rolling values though.\n",
    "Here we grab the biggest purchase each person made over each 30 day window.\n",
    "\n",
    "Notice that a given value will stick around for up to 30 days, but will get replaced if a bigger purchase occurs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxes_df = pivot_df.rolling(30, min_periods=1).max()\n",
    "maxes_df.iloc[:31]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now Make it Pretty\n",
    "\n",
    "Since we did most of the work in pandas already to shape the data, the Data Analysis of it should be more straightforward\n",
    "\n",
    "We'll use a helper function to do one final transformation that applies to almost all our datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_df_for_display(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    return df.divide(100).reset_index().melt(\"purchased_date\")\n",
    "\n",
    "prepped_cumulative = prep_df_for_display(cumulative)\n",
    "prepped_cumulative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems like it's undoing a lot of work we've already done, but this Long Format is generally easier for plotting software to work with.\n",
    "\n",
    "In this case we keep `purchased_date` as a column (not index), get a value column called `value`, and a column we can use for trend highlighting which is `purchased_by`\n",
    "\n",
    "After that, plotly express provides the easiest (but not most performant) visualizations in my experience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "px.line(\n",
    "    prepped_cumulative,\n",
    "    x=\"purchased_date\",\n",
    "    y=\"value\",\n",
    "    color=\"purchased_by\",\n",
    "    labels={\"value\": \"Cumulative Dollars Spent\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more of the plotting and charting, check it out [live on streamlit!](https://share.streamlit.io/gerardrbentley/roommate-ledger/main/app.py)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9b22f3d865ff1ad08335d823a735a6bb7c186403b86aac04963e321aec0c37cf"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
